{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5911a3-4d5a-4163-9c06-6c3e8e15bb55",
   "metadata": {},
   "source": [
    "### From Linear Regression to Logistic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111c509-798f-488a-a06b-d04db46765a2",
   "metadata": {},
   "source": [
    "Linear Regression and Logistic Regression (classification): \n",
    "In both, we’re trying to learn a function that approximates $P(y|x)$. They both make the assumption that $P(y|x)$ can be approximated as some function of a linear combination of input features where $y = \\mathbf{w}^T \\mathbf{x} + b$. The key difference lies in that Logistic Regression applies a sigmoid function to this linear combination of input features $\\sigma(\\mathbf{w}^T \\mathbf{x} + b)$. This makes the models differ in their outputs, the interpretation of their parameters, and the loss functions that could be used to optimise them:\n",
    "\n",
    "**The outputs:**\n",
    "- Linear regression: output is a continuous value from $-\\infty$ to $+\\infty$ representing the predicted value of $y$ ($y = \\mathbf{w}^T \\mathbf{x} + b$)\n",
    "- Logistic regression: output is a probability value between 0 and 1, representing the likelihood of an instance belonging to a specific class.\n",
    "\n",
    "**Interpretation of coefficients:**\n",
    "   - In linear regression, the coefficients represent the change in the output variable for a one-unit change in the corresponding input feature, assuming all other features remain constant.\n",
    "   - In logistic regression, the coefficients represent the change in the log-odds (logit) of the output variable for a one-unit change in the corresponding input feature. The coefficients can be exponentiated to obtain the odds ratios, which represent the multiplicative change in the odds of the output variable.\n",
    "\n",
    "**Loss function:**\n",
    "   - Linear regression typically uses the mean squared error (MSE) or mean absolute error (MAE).\n",
    "   - Logistic regression uses the binary cross-entropy loss function (also known as log loss) to measure the dissimilarity between the predicted probabilities and the actual class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8e969-f9d7-4c6c-b859-e0aef18b87dc",
   "metadata": {},
   "source": [
    "**The Math**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46307e2-e77e-4890-9c98-0cb2ed351f3d",
   "metadata": {},
   "source": [
    "Logistic Regression (classification) does binary classification. Binary classification takes an input $x$ and tries to predict if it belongs to a certain class $Y$ ($y = 1$ if it belongs or not $y = 0$). The output of logistic regression should be then the probability that label $y = 1$. Each output / label therefore will be interpreted as a Bernoulli r.v. $Y \\sim Bern(p)$.\n",
    "\n",
    "$P(y|x)$ for bernoulli:\n",
    "$$P(x = x) = p \\quad \\text{if } x = 1$$\n",
    "$$P(x = x) = 1 - p \\quad \\text{if } x = 0$$\n",
    "In one line:\n",
    "$$P(x = x) = p^x \\cdot (1 - p)^{(1-x)} \\quad \\text{(x could be 1 or 0 and it will give u the 2 equations above)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ad84b-b74e-436f-bae3-c6d16e7b80e7",
   "metadata": {},
   "source": [
    "Logistic regression will try to model $P(y|x)$ i.e., $P(y = 1 | x = x)$ and $P(y = 0 | x = x)$.\n",
    "It assumes $P(y | x)$ can be approximated as a sigmoid function applied to a linear combination of input features.\n",
    "\n",
    "So, probability of a single data point in logistic regression is:\n",
    "$$P(Y = 1 | X = x) = \\sigma(\\theta_0x_0 + \\theta_1x_1 + \\ldots + \\theta_mx_m)$$\n",
    "\n",
    "Where $x$ (input) is a feature vector of $m$ dimensions e.g., House [size, num rooms, num bathrooms, price, …] and the model parameters are of size $m$ equal to input features.\n",
    "\n",
    "and if we always have $x_0 = 1$ then we can write\n",
    "$$P(Y = 1 | X = x) = \\sigma(\\theta^T X)$$\n",
    "\n",
    "and\n",
    "$$\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$$\n",
    "\n",
    "So, analogous to what we did with Bernoulli\n",
    "$$P(Y = 1 | X = x) = \\sigma(\\theta^T X)$$\n",
    "$$P(Y = 0 | X = x) = 1 - \\sigma(\\theta^T x)$$\n",
    "\n",
    "*each of these is the $p$ in $Bern(p)$*\n",
    "\n",
    "In one line: Probability of one datapoint (the equation form of the probability mass function of a Bernoulli):\n",
    "$$P(Y = y | X = x) = \\sigma(\\theta^T X)^y + (1 - \\sigma(\\theta^T X))^{(1-y)}$$\n",
    "\n",
    "Consider we have “n” examples / data points, the likelihood of all data points / independent training examples is:\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^{n} P(Y=y^{(i)} | X=x^{(i)})$$\n",
    "=> substitute Bernoulli equation\n",
    "$$L(\\theta) = \\prod_{i=1}^{n} \\left[\\sigma(\\theta^T x^{(i)})^{y^{(i)}} \\cdot (1 - \\sigma(\\theta^T x^{(i)}))^{(1-y^{(i)})}\\right]$$\n",
    "\n",
    "Take its log to convert multiplication to addition:\n",
    "$$LL(\\theta) = \\sum_{i=1}^{n} \\left[y^{(i)} \\cdot \\log(\\sigma(\\theta^T x^{(i)})) + (1-y^{(i)}) \\cdot \\log(1-\\sigma(\\theta^T x^{(i)}))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff198b9-59e1-45db-aed1-dd39ce3c1df9",
   "metadata": {},
   "source": [
    "Now that we have a function for log-likelihood, we simply need to chose the values of theta that maximize it. We can find the best values of theta by using an optimization algorithm (gradient descent). However, in order to use an optimization algorithm, we first need to know the partial derivative of log likelihood with respect to each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37456bde-2138-47dd-84cb-c20055e0fecf",
   "metadata": {},
   "source": [
    "**How to take derivative of this complicated function using chain rule:**\n",
    "$$LL(\\theta) = \\sum_{i=1}^{n} \\left[y^{(i)} \\cdot \\log(\\sigma(\\theta^T x^{(i)})) + (1-y^{(i)}) \\cdot \\log(1-\\sigma(\\theta^T x^{(i)}))\\right]$$\n",
    "\n",
    "Since derivative of sum = sum of derivatives, we’ll focus on derivative of one example (since they’ll all sum)\n",
    "$$LL(\\theta) = y \\log(\\sigma(\\theta^T x)) + (1−y) \\log(1−\\sigma(\\theta^T x))$$\n",
    "\n",
    "Since we know:\n",
    "$$p=\\sigma(\\theta^T x)$$\n",
    "and\n",
    "$$z=\\theta^T x$$\n",
    "\n",
    "Let’s simplify it:\n",
    "$$LL(\\theta) = y \\log(p) + (1−y) \\log(1−p)$$\n",
    "\n",
    "By the chain rule:\n",
    "$$\\frac{\\partial LL(\\theta)}{\\partial \\theta_j} = \\frac{\\partial LL(\\theta)}{\\partial p} \\cdot \\frac{\\partial p}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\theta_j}$$\n",
    "\n",
    "Now find each of these terms:\n",
    "\n",
    "$p = \\sigma(z)$ -> $\\frac{\\partial p}{\\partial z} = \\sigma(z)[1−\\sigma(z)]$ (By taking the derivative of the sigmoid)\n",
    "\n",
    "$z = \\theta^T x$ -> $\\frac{\\partial z}{\\partial \\theta_j} = x_j$ (Only $x_j$ interacts with $\\theta_j$)\n",
    "\n",
    "$LL(\\theta) = y \\log(p) + (1−y) \\log(1−p)$ -> $\\frac{\\partial LL(\\theta)}{\\partial p} = \\frac{y}{p} - \\frac{1−y}{1−p}$\n",
    "\n",
    "Each of those derivatives was much easier to calculate. Now we simply multiply them together.\n",
    "\n",
    "$$\\frac{\\partial LL(\\theta)}{\\partial \\theta_j} = \\left[\\frac{y}{p} - \\frac{1−y}{1−p}\\right] \\cdot \\sigma(z)[1- \\sigma(z)] \\cdot x_j$$\n",
    "By substituting in for each term = $\\left[\\frac{y}{p} - \\frac{1−y}{1−p}\\right] \\cdot p[1- p] \\cdot x_j$ \n",
    "\n",
    "Since $p = \\sigma(z)$ = $\\left[y(1-p) - p(1-y)\\right] \\cdot x_j$ \n",
    "\n",
    "Multiplying in = $\\left[y - p\\right]x_j$ Expanding = $\\left[y - \\sigma(\\theta^T x)\\right]x_j$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1bc101-ac37-4d7e-997c-c4cd3f66af9b",
   "metadata": {},
   "source": [
    "*NB:*\n",
    "\n",
    "To find the derivative of $\\log(1 - p)$ with respect to $p$, we can use the chain rule:\n",
    "\n",
    "$$\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "In this case, let's define:\n",
    "- $f(x) = \\log(x)$\n",
    "- $g(p) = 1 - p$\n",
    "\n",
    "Then, $\\log(1 - p) = f(g(p))$\n",
    "\n",
    "**Step 1:** Find the derivative of $f(x)$ with respect to $x$.\n",
    "- $f(x) = \\log(x)$\n",
    "- $f'(x) = \\frac{1}{x}$\n",
    "\n",
    "**Step 2:** Find the derivative of $g(p)$ with respect to $p$.\n",
    "- $g(p) = 1 - p$\n",
    "- $g'(p) = -1$\n",
    "\n",
    "**Step 3:** Apply the chain rule.\n",
    "$$\\frac{d}{dp} \\log(1 - p) = f'(g(p)) \\cdot g'(p) = \\frac{1}{1 - p} \\cdot (-1) = -\\frac{1}{1 - p}$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\frac{d}{dp} \\log(1 - p) = -\\frac{1}{1 - p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcdcb85-e169-4eec-8ae8-74c2e39bdeab",
   "metadata": {},
   "source": [
    "### In code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043151d-7295-45e5-8a44-593b4f04bb86",
   "metadata": {},
   "source": [
    "First, we'll generate a synthetic dataset to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb6feba5-e585-47a9-a2c9-394f5c735108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ade543b2-37e1-4ccb-9e89-9bfe0ba1f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_samples, num_features):\n",
    "    xs = []; ys = []\n",
    "    for _ in range(num_samples):\n",
    "        features = [random.uniform(0,1) for _ in range(num_features)]\n",
    "        xs.append(features)\n",
    "        ys.append(random.choice([0,1]))\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8a8fd9c-2365-4758-b233-b27ee99185fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 2\n",
    "num_samples = 10\n",
    "xs, ys = generate_dataset(num_samples, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6aea9f4e-3e16-459d-a9b1-72106df790c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0.2605950289782042, 0.5574727514210197], 0),\n",
       " ([0.5721455288417747, 0.06878027736984715], 1),\n",
       " ([0.5143613269858414, 0.3553292437524699], 1),\n",
       " ([0.9761390402284876, 0.7078873052442007], 0),\n",
       " ([0.9655319251899727, 0.5652320334915052], 1),\n",
       " ([0.6189379895865476, 0.02696606885399666], 0),\n",
       " ([0.6315529568513656, 0.5676285036286293], 0),\n",
       " ([0.3470826325291636, 0.7144521743133823], 1),\n",
       " ([0.4989689701228194, 0.3912761508767272], 0),\n",
       " ([0.4880566177936432, 0.006889842454781747], 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(xs, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd6a41fe-b85c-4481-9e46-ae79f71b0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_weights(num_samples, num_features):\n",
    "    w = []\n",
    "    for _ in range(num_samples):\n",
    "        w.append([random.random() for _ in range(num_features)])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f4ee28c-43c4-4800-8d76-cfc9ef3e6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = generate_model_weights(num_samples, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c9de5f8-dfb2-4aea-8cb6-2dc175fa572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cbb5b70-fc1d-4098-a2c5-5597d03f578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is sigmoid(wx)\n",
    "def sigmoid(z): return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99cbbcb8-bb69-4194-8659-6131ff2b7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(w, x):\n",
    "    res = []\n",
    "    for x_vec, w_vec in zip(x, w):\n",
    "        example = []\n",
    "        for xi,wi in zip(x_vec, w_vec):\n",
    "            example.append(xi*wi)\n",
    "        res.append(sum(example))\n",
    "    return res       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eac90117-ce6e-4c31-93c8-3c5a2a78a757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36465061406789456,\n",
       " 0.10428776033012813,\n",
       " 0.11900962949692745,\n",
       " 0.5000708141638529,\n",
       " 0.8398581744159634,\n",
       " 0.10227433501167904,\n",
       " 0.5459675700646139,\n",
       " 0.6180184060811562,\n",
       " 0.5009334195997212,\n",
       " 0.22400145924726633]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product(xs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f27f88da-1fc9-4846-9b2c-fe7086392775",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [sigmoid(i) for i in dot_product(weights, xs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50b4e6ad-16a3-4329-826c-a5d85b221e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(y, p):\n",
    "    N = len(p)\n",
    "    total_ll = 0\n",
    "    for i in range(N):\n",
    "        ll = y[i] * math.log(p[i]) + (1 - y[i])*(math.log(1 - p[i]))\n",
    "        total_ll += ll\n",
    "    return total_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3793afa-132d-4365-9842-f0be0f0326b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.244542532770307"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood(ys, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dce7a82a-40c1-4e5a-87b2-f3b6cc88932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient computation and backpropagation\n",
    "for n, x in enumerate(xs):\n",
    "    for m, xi in enumerate(x):\n",
    "        gradient = (ys[n] - ps[n]) * xi\n",
    "        weights[n][m] += -1.0 * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3ea968a-a5fd-425f-81a9-b1a7be2c33e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log Likelihood: 7.1381280818011685\n",
      "Negative Log Likelihood: 1.938974948429435\n",
      "Negative Log Likelihood: 1.0882342944665075\n",
      "Negative Log Likelihood: 0.7470847634565279\n",
      "Negative Log Likelihood: 0.5659523917625765\n"
     ]
    }
   ],
   "source": [
    "# full pass\n",
    "weights = generate_model_weights(num_samples, num_features)\n",
    "\n",
    "for i in range(50):\n",
    "    # forward pass\n",
    "    ps = [sigmoid(i) for i in dot_product(weights, xs)]\n",
    "    # calculate loss\n",
    "    ll = log_likelihood(ys, ps)\n",
    "    # backpropagation\n",
    "    for n, x in enumerate(xs):\n",
    "        for m, xi in enumerate(x):\n",
    "            gradient = (ys[n] - ps[n]) * xi\n",
    "            weights[n][m] += 1.0 * gradient\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Negative Log Likelihood: {-ll}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d58b62f-544a-46c7-aeaf-c10278c4c0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.05667587183880696),\n",
       " (1, 0.9392469086860377),\n",
       " (1, 0.9499221535198927),\n",
       " (0, 0.014191869518466701),\n",
       " (1, 0.9840701024407983),\n",
       " (0, 0.05527682744732184),\n",
       " (0, 0.028831062184110582),\n",
       " (1, 0.9677331060701538),\n",
       " (0, 0.05416766887041442),\n",
       " (1, 0.9174002515459945)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(ys, ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab45c680-7a38-456d-9432-b38bf9603ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 0, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(p) for p in ps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6af85e6-9c86-4e65-a1c7-b2a73e21441f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1, 0, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
