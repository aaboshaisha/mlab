{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261b0d6f-1744-4884-b8e0-3a7d039df3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdc39a-ae71-429f-888f-c4ba13ce24d5",
   "metadata": {},
   "source": [
    "Our goal here is to start exploring DL by building `makemore`. `makemore` is a model the makes more of what u give it. We'll give it names and it will make more name-like words. \n",
    "\n",
    "We'll start by building it as a character-level language model ie: a model that given a character will predict the next character. \n",
    "\n",
    "So, we'll start with some text corpus (in our case a file with a list of 32k names) and we want to train some model (whatever that is) which will somehow extract information about letter / character statistics from this file and use that to predict the next character.\n",
    "\n",
    "In some way, we want the model: `P(character 2 | character 1) = ... ` eg `P(\"e\" | \"m\")`\n",
    "\n",
    "There are 26 characters in English language. Our model will thus need to output a distribution over the 26 characters for ch2 (given any ch 1), and assign high probability to specific characters based on the statistics of the characters in the provided file it trains on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c2c21-63b2-4bd2-9c04-74501fe9566c",
   "metadata": {},
   "source": [
    "**What can we do?** \n",
    "\n",
    "The easiest thing to do initially is to count: let's count how many times any character j follows character i. We'll build this as a table of frequency counts (the rows represent 1st cc, columns represent 2nd cc and any cell represents `P(ch2 | ch1)`). We'll then normalize this into probabilities. All our model needs to do now to generate new words is to sample from this table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b0a68e-9c11-4412-a0ae-2fc831c200ae",
   "metadata": {},
   "source": [
    "NB: This is a classification problem. We're outputting the probability that ch2 belongs to one of 26 labels.\n",
    "\n",
    "Our model will try to learn `P(Y|X)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa5971-1368-4955-8511-0ebf1fa94b90",
   "metadata": {},
   "source": [
    "NB: To know where to start and where to stop, we'll add a special token (character) '.' to denote starting and stopping. It will be part of our table such that \"e.\" gives probability that \"e\" is a terminal character and similarly \"a.\" gives probability that a name starts with \"a\". \n",
    "\n",
    "We'll call this model a \"Bigram\" model (since it takes into account information from 2 characters only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09363c8-38d9-43dc-9a5a-4a8b8e2d444b",
   "metadata": {},
   "source": [
    "This will be our baseline model and we'd have learned it by explicitly counting. \n",
    "\n",
    "The other way we could try is to \"learn\" these probability distributions from the data using a neural network. Our network will take as input one character and output probabilities over 27 next characters (probability distribution). \n",
    "\n",
    "Suppose we start with a 1 layer NN in which each neuron performs `tanh(wx + b)`. How can we make these into probabilities? (*pay attention here, we're saying \"make into probabilities\" because this is a construction process. We'll \"impose\" certain semantics on the outputs of the NN by mathematical manipulation ie: we will \"transform\" the outputs to probabilites by various math operations)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa814173-3783-4965-80f7-8918f1b09194",
   "metadata": {},
   "source": [
    "The outputs after tanh will be between -1 and 1. We'll interpret these as the `log` of the frequency counts a.k.a `logits`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18132dde-d491-48a0-ad12-1be2f3ed9479",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>`logits`: Click to expand or collapse</summary>\n",
    "\n",
    "\n",
    "In classification problems and language modeling, the term \"logits\" refers to the raw, unnormalized outputs of a neural network before they are passed through an activation function like softmax. Logits are often used as an intermediate step in the output layer of a neural network.\n",
    "\n",
    "In language modeling, the goal is to predict the probability distribution over the next word given the previous words. One way to estimate these probabilities is by counting the frequency of each word appearing after a given context in the training data. The raw frequency counts can be converted into probabilities by dividing each count by the total number of words.\n",
    "\n",
    "Instead of directly using the frequency counts, we often take the logarithm of the counts. This is because the frequency of words in a language follows a power law distribution (Zipf's law), where a few words appear very frequently, and many words appear rarely. By taking the log of the counts, we can compress the dynamic range and make the values more manageable.\n",
    "\n",
    "The output layer of a neural network for language modeling typically has one neuron for each word in the vocabulary. The activations of these neurons can be interpreted as the log-frequencies (or unnormalized log-probabilities) of each word given the input context. By applying the softmax function to the logits, we can convert them into normalized probabilities that sum up to 1.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78566c9-16df-4ba0-9a52-c6de7fb10af3",
   "metadata": {},
   "source": [
    "To make `logits` to frequencies / counts we can do `exp(logits)`.\n",
    "\n",
    "We can then convert counts to probabilities by normalizing. \n",
    "\n",
    "These 2 steps will look like: $$\\frac{exp(z_j)}{\\sum_i exp(z_i)}$$\n",
    "\n",
    "And this is called a `softmax` function. \n",
    "\n",
    "We then want to find the best parameters `w` for the model that make this data likely `P(data \"X\"|w)`. We'll do it using MLE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62082d93-9ecd-48b0-b410-8ddf4e558df1",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4b4809-0ac7-4825-a5b5-2d65e09ef824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first load the names into a list\n",
    "words = open(\"names.txt\").read().split()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079fcddf-1464-4899-8f9e-e0b6ab2d0ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a vocab from them\n",
    "vocab = list(set(\"\".join(words)))\n",
    "vocab.sort()\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04790405-756f-4d33-a5c5-8280235266cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map from letters to numbers and vice versa\n",
    "stoi = {s:i+1 for i,s in enumerate(vocab)}\n",
    "# add special start / end token\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2f3aec-0836-431f-801d-56a1f98f9e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e m\n",
      "m m\n",
      "m a\n"
     ]
    }
   ],
   "source": [
    "# example : create a bigram \n",
    "for w in words[:1]:\n",
    "    for ch1,ch2 in zip(w, w[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b7b5d-fdde-4c75-a07e-1663adac1493",
   "metadata": {},
   "source": [
    "count table: a 2D tensor where col1 = all ch, row1 = all ch, each cell is frequency count of ch1,ch2\n",
    "```\n",
    " . a b c d e\n",
    ".\n",
    "a\n",
    "b\n",
    "c\n",
    "d\n",
    "e\n",
    "```\n",
    "in numbers\n",
    "```\n",
    "  0 1 2 . . 26\n",
    "0\n",
    "1\n",
    "2\n",
    ".\n",
    ".\n",
    "26\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86837ad8-701c-44df-8d23-25ec986127d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the full bigram\n",
    "B = torch.zeros(27,27, dtype=torch.int32)\n",
    "for w in words:\n",
    "    w = '.' + w + '.'\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        i = stoi[ch1]; j = stoi[ch2];\n",
    "        B[i,j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec2c696-1826-4900-b605-c3047c344a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b09a6de-e1e8-4ff9-b47e-b8bc226ba2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a generator object in torch to get the same results\n",
    "generator = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b14bbac-a035-49ed-82f6-c902fd0b5fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 27]), torch.Size([27]), torch.Size([27, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape, B.sum(axis=1).shape, B.sum(axis=1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb674709-9345-4d43-9c75-e5b060b15b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), tensor(3.0222))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert frequency counts to probabilities\n",
    "P = B / B.sum(axis=1, keepdim=True) # keepdim allows proper broadcasting\n",
    "P[0].sum(), P[:,0].sum() # confirm broadcasting done right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c72da70-7915-4538-a015-8183b3164b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 'j')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can use p to sample. Our model is a classifier of 27 labels ie prob that ch belongs to any \n",
    "# of 27 labels / classes => it's a multinomial distribution\n",
    "ch = torch.multinomial(input=P[0], num_samples=1, replacement=True, generator=generator).item()\n",
    "ch, itos[ch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7f38-bca7-49d7-9747-bb698ebbed82",
   "metadata": {},
   "source": [
    "We start with `input=p[0]` because our start token for any word is `. = 0`\n",
    "\n",
    "`replacement=True` since we want to allow for repeat letters\n",
    "\n",
    "We can then use that sampled 1st character to pick the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a3758e0-3c6e-4e6c-afc2-ef7b733ad1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 'u')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch2 = torch.multinomial(input=P[ch], num_samples=1, replacement=True, generator=generator).item()\n",
    "ch2, itos[ch2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b7e9443-e4bf-4e17-a937-1c8a3cfa17be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "# let's do in a loop\n",
    "for _ in range(5):\n",
    "    sample = ''\n",
    "    ch= torch.multinomial(input=P[0], num_samples=1, replacement=True, generator=generator).item()\n",
    "    sample += itos[ch]\n",
    "    while ch != 0:\n",
    "        ch = torch.multinomial(input=P[ch], num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample += itos[ch]\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cd40eac-e112-4917-8ee1-3871054c7552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juee.\n",
      "ksahnaauranilevias.\n"
     ]
    }
   ],
   "source": [
    "# or to avoid repetition\n",
    "for _ in range(5):\n",
    "    sample = ''\n",
    "    ch = 0\n",
    "    while True:\n",
    "        p = P[ch]\n",
    "        ch = torch.multinomial(input=p, num_samples=1, replacement=True, generator=generator).item()\n",
    "        sample += itos[ch]\n",
    "        if ch == 0:\n",
    "            break\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29398183-4222-4acd-acc4-14f71f608a2d",
   "metadata": {},
   "source": [
    "**Now, let's see how good this model is: MLE**\n",
    "\n",
    "MLE estimates `P(data | parameters)` what parameters make this data most likely.\n",
    "\n",
    "So, to compute MLE, we want to compute this number. We'll go ove the data (names list) and compute likelihood for each bigram. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e953de7a-27fb-42aa-bd69-2f9fafad27a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-559891.7500)\n",
      "nll=tensor(559891.7500)\n",
      "2.454094171524048\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0\n",
    "n_bigrams = 0 # to compute average likelihood later\n",
    "for w in words:\n",
    "    w = '.' + w + '.'\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        i = stoi[ch1]\n",
    "        j = stoi[ch2]\n",
    "        p = P[i, j]\n",
    "        logp = p.log()\n",
    "        log_likelihood += logp\n",
    "        n_bigrams += 1\n",
    "        \n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n_bigrams}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15815e0-e7e3-43a5-bcbe-75971e7e511c",
   "metadata": {},
   "source": [
    "### Use 1-layer NN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d59ec-c68f-484e-b12e-edf3cbe34bff",
   "metadata": {},
   "source": [
    "We'll make another model that uses a NN. In fact, it uses a simple one layer NN which in fact is just logistic regression. It takes as input the first character and outputs a probability distribution above. It tries to learn the parameters (the frequency table) from the data. \n",
    "\n",
    "To do so, we need to provide it with data in a format it could learn from. \n",
    "- Input will be ch1\n",
    "- Label will be ch2\n",
    "\n",
    "But, NNs can't take integers as input. We'll do one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18a2a9e1-2ed8-442e-b31e-0b2a4c5e4b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['.', 'e', 'm', 'm', 'a'], ['e', 'm', 'm', 'a', '.'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset \n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    w = '.' + w + '.'\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        xs.append(ch1)\n",
    "        ys.append(ch2)\n",
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06a10e21-80ea-47db-9807-a4d09b5e20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset \n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    w = '.' + w + '.'\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs[:5], ys[:5], len(xs), len(ys)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3aa9a9b1-3fe4-46f9-9809-83b2bc2e85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22c08490-99ae-4473-b1a9-16fc3a7030e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f069acd-5e82-4c03-a911-03b530e08713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 27]), torch.Size([228146]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c643b0c5-4ed4-4add-92e5-eead9b1e9c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efcc74dc-544f-4d17-9bbd-50a474231826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "W = torch.randn(27, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "054044ae-52c2-4839-9d00-c3b430fdd53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "logits = xenc @ W\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97d509d6-fef4-46ea-815e-0f5d44bdca86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0811, -1.8241,  0.3685, -0.1218, -0.6174,  0.4972, -0.4562,  1.3473,\n",
       "         0.0225, -0.8140, -0.6009,  0.7199,  0.3964,  0.8327, -0.0620, -0.6234,\n",
       "         0.0358, -0.3556, -0.3480,  0.4521, -0.9783, -1.9683, -0.9615,  1.3405,\n",
       "         0.7477, -0.6550, -1.7136])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these outputs are logits\n",
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcd0f9d2-dc98-4c9c-879f-c101070ca5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to counts via exp\n",
    "counts = logits.exp()\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e41090b8-586a-46d3-bb42-114a2cc2e88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize to probabilities\n",
    "P = counts / counts.sum(dim=1, keepdim=True)\n",
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381160a-c696-4376-ba73-981b6a6d4d83",
   "metadata": {},
   "source": [
    "Compute the loss: The model outputs a prob distribution of the likelihood over the 27 characters. We want it to assign highest probability to the correct label (a perfect model would assign highest probability (= 1) to all correct labels).\n",
    "\n",
    "The loss to compute is how far off is the probability of predicted label from 1. \n",
    "\n",
    "We'll compute the likelihood the model assigns to ALL examples then try to minimize negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d09d0985-6c74-4243-b582-272b8338a89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the correct labels\n",
    "ps = P[torch.arange(xs.nelement()), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad8d82bb-422e-4b81-a805-a1002f4eae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logps = ps.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6c2b396-0fcc-428f-adde-259263feee78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7685)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll = -logps.sum() / xs.nelement()\n",
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4d258b7-fdb3-48ee-9f85-42591c8feb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.831210136413574\n",
      "loss: 2.6707763671875\n",
      "loss: 2.5668914318084717\n",
      "loss: 2.529362201690674\n",
      "loss: 2.509660482406616\n"
     ]
    }
   ],
   "source": [
    "# let's train the model to get a better loss\n",
    "W = torch.randn((27,27), generator=generator, requires_grad=True)\n",
    "for i in range(50):\n",
    "    #forward pass\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp() #logits to counts\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True) #counts to probs\n",
    "    loss = -probs[torch.arange(xs.nelement()), ys].log().mean() #mean nll\n",
    "\n",
    "    \n",
    "    W.grad = None #zero grads\n",
    "    loss.backward() #backprop\n",
    "    W.data += -50.0 * W.grad # weight update\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e36866-078e-4d5a-bf48-9684c5399ccc",
   "metadata": {},
   "source": [
    "**let's sample from this model**\n",
    "\n",
    "Now that we have the trained model, we can sample from it as we did before. \n",
    "\n",
    "This model works by taking one character as input and outputting the next. To sample from it we need to follow thatprocess: - We need to start with character `.`\n",
    "- We then one-hot-encode it and pass through the network\n",
    "- It outputs a activations over next 27 possible characters.\n",
    "- Turn them to counts then probabilities\n",
    "- Use the probabilities to sample from the multinomial as before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "110b91fd-84c2-472b-87de-145415b0db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "pan.\n",
      "ay.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "generator = torch.Generator().manual_seed(2147483647)\n",
    "# sampling\n",
    "for _ in range(5):\n",
    "    ix = 0\n",
    "    word = ''\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum()\n",
    "        ix = torch.multinomial(input=p, num_samples=1, replacement=True, generator=generator).item()\n",
    "        word += itos[ix]\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36088fff-9e0d-4936-bf17-ddff70213ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
