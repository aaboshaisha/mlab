{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc39c8be-2566-4c70-95d9-5160901f4390",
   "metadata": {},
   "source": [
    "### Neural Networks as math expressions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0025327-0b99-4e9b-b512-2cf7cd6168ca",
   "metadata": {},
   "source": [
    "- Neural Networks are just mathematical expressions. \n",
    "- They take as input (data & parameters) and output predictions which enter some loss function to measure (compare) them against true values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9436e5f-a5ac-4c6b-a82c-e9889d42643b",
   "metadata": {},
   "source": [
    "A very simple example of a neural network might be a single neuron performing a linear regression task, which could be represented mathematically as:\n",
    "\n",
    "$ y = wx + b $\n",
    "\n",
    "We could add a non-linear activation function, such as the sigmoid function. The mathematical expression would then be:\n",
    "\n",
    "$ y = \\sigma(wx + b)$\n",
    "\n",
    "where $\\sigma$ is the sigmoid activation function defined as:\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d973f86-192c-4669-ad7e-b4f735d1ff52",
   "metadata": {},
   "source": [
    "- We want to tweak the parameters (knobs) to reduce the loss (and thus make NNs prediction close to truth).\n",
    "- We know by how much to tweak parameters and in what direction via gradients (derivatives).  \n",
    "- Backpropagation is an algorithm that computes the gradient / derivative of a loss function w.r.t parameters of the NN-math-expression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb966b-381a-4161-93d6-89c453989f25",
   "metadata": {},
   "source": [
    "### From Derivatives to Computation Graphs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5778d1-4b85-4943-8634-2fa26529a159",
   "metadata": {},
   "source": [
    "#### Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087cd2d-e11b-4a64-be8b-4719dacc20b7",
   "metadata": {},
   "source": [
    "Once we know how to compute derivatives for simple functions, the next step is to compute derivatives for functions combinations. \n",
    "\n",
    "There are 3 ways to combine functions:\n",
    "\n",
    "1. Sum: `h(x) = f(x) + g(x)`\n",
    "2. Product: `h(x) = f(x) * g(x)` \n",
    "3. Function composition (one inside the other) `h(x) = f(g(x))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdd1f9-359b-41ff-a967-f32b16336a7e",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> Their derivatives are given by (click to expand / collpase): </summary>\n",
    "\n",
    "1. Sum rule: derivative of a sum = sum of derivatives >> easy to imagine: since we sum the \"values\" of both functions at each point, if we nudge each to produce some change, we'll end up summing the changes.\n",
    "2. Product rule: dfirst x second + dsecond x first >> think of each function's value as representing a side of a square / rectangle.\n",
    "\n",
    "```\n",
    "       x^2\n",
    "   +--------+\n",
    "   |        |\n",
    "sin(x)      sin(x)\n",
    "   |        |\n",
    "   +--------+\n",
    "       x^2\n",
    "```\n",
    "and as we nudge x by dx such that -> x + dx\n",
    "\n",
    "```\n",
    "+---------- x^2 ---------dx^2---+\n",
    "|                      |        |\n",
    "|                      |        | sin(x)\n",
    "|----------------------| -------| \n",
    "|                      |        | dsin(x)\n",
    "+----------------------+--------+\n",
    "            x^2          dx^2\n",
    "```\n",
    "ie: the resulting change will be the area of the 2 new (top right and bottom left) rectangles:\n",
    "- x^2 * dsin(x)\n",
    "- sin(x) * dx^2\n",
    "\n",
    "(we ignore the small lower square dsin(x) * dx^2 since dx is supposed to be very small so in the limit ->0 this small square vanishes\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7f167-8128-4851-a180-7e87122326ea",
   "metadata": {},
   "source": [
    "3. Function composition:\n",
    "\n",
    "From [Chain rule - Wikipedia](https://en.wikipedia.org/wiki/Chain_rule)\n",
    "As put by George F. Simmons:\n",
    "\n",
    "> \"If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 × 4 = 8 times as fast as the man.\"[^1]\n",
    "\n",
    "Let $z, y, x$ be the (variable) positions of the car, the bicycle, and the walking man, respectively. The rate of change of relative positions of the car and the bicycle is\n",
    "\n",
    "$ \\frac{dz}{dy} = 2. $\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$ \\frac{dy}{dx} = 4. $\n",
    "\n",
    "So, the rate of change of the relative positions of the car and the walking man is\n",
    "\n",
    "$ \\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} = 2 \\cdot 4 = 8. $\n",
    "\n",
    "The rate of change of positions is the ratio of the speeds, and the speed is the derivative of the position with respect to time; that is,\n",
    "\n",
    "$ \\frac{dz}{dx} = \\frac{\\frac{dz}{dt}}{\\frac{dx}{dt}}, $\n",
    "\n",
    "or, equivalently,\n",
    "\n",
    "$ \\frac{dz}{dt} = \\frac{dz}{dx} \\cdot \\frac{dx}{dt}, $\n",
    "\n",
    "which is also an application of the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20f537-e25a-4d98-9a0f-301f00e3893f",
   "metadata": {},
   "source": [
    "Now, Suppose we have the following math expression:\n",
    "\n",
    "```\n",
    "e = a * b\n",
    "d = e + c\n",
    "L = d * f\n",
    "```\n",
    "\n",
    "We want to calculate the derivative of `L` w.r.t `a & b` but also all other components `c, d, e, f`\n",
    "\n",
    "How can we compute `dL/da` and similarly `dL/db`? \n",
    "\n",
    "```\n",
    "de/da = b (since a is scaled '*' by b, if we nudge a a little bit, the result is also scaled by b\n",
    "similarly de/db = a\n",
    "\n",
    "dd/de = de/de + dc/de = 1 + 0 = 1\n",
    "similarly dd/dc = 1\n",
    "\n",
    "dL/dd = f\n",
    "dL/df = d\n",
    "```\n",
    "What the chain rule then tells us is:\n",
    "`dL/da = de/da x dL/de `\n",
    "\n",
    "And we can do the same for all intermediate values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ffd1a1-65ec-448e-992f-8382eb215eba",
   "metadata": {},
   "source": [
    "#### Computational Graphs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077293f-a895-41df-8f61-fdbad5d34fbd",
   "metadata": {},
   "source": [
    "**How can we turn the pen and paper math into computation inside the machine?**\n",
    "\n",
    "Let's think about functional programming: [Source](https://link.springer.com/chapter/10.1007/978-1-4842-8853-5_6)\n",
    "- A function takes an input and then produces an output. \n",
    "- The input of a function can be the output of another function.\n",
    "- If we view a function as one node in a graph, and its input and output as incoming and outgoing links to other functions, respectively, as the computation continues, these functions are **chained** together to form a directed acyclic graph (DAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cc770-d90d-4823-950e-a23717745b55",
   "metadata": {},
   "source": [
    "So, our functions will look like:\n",
    "```\n",
    "e = a * b\n",
    "d = e + c\n",
    "L = d * f\n",
    "```\n",
    "```\n",
    "a ---  \n",
    "      \\  \n",
    "       (*)-- e --(+)-- d --(*)-- L\n",
    "      /           |         |\n",
    "b ---             |         |\n",
    "                  |      f--\n",
    "              c---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e2a64-4855-4042-ae75-bf693d08f2b4",
   "metadata": {},
   "source": [
    "And we can see that conveniently the **chain** rule allows us to break the complex computations of derivatives into simpler ones that can just be multiplied together. \n",
    "\n",
    "**How?**\n",
    "\n",
    "Our goal is to compute the derivative of `L` w.r.t all other input values ie: `dL/dd, dL/df, dL/de, dL/dc, dL/da, dL/db`\n",
    "\n",
    "At `L`, we can find `dL/dd` and `dL/df`. We can then pass `dL/dd` to `d` node. \n",
    "\n",
    "Similarly at `d` we can find `dd/de` and `dd/dc`. We then use `dL/dd` (passed from above) to compute:\n",
    "- `dL/de = dL/dd x dd/de`\n",
    "- `dL/dc = dL/dd x dd/dc`\n",
    "\n",
    "And pass these results to `e` then at `e` we can find `de/da` and `de/db` and similarly compute `dL/da, dL/db`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85210b1-564a-476c-9c2c-21c9a1479ba2",
   "metadata": {},
   "source": [
    "### Computational implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee099ffa-b57d-415f-85a9-bfa7225e370b",
   "metadata": {},
   "source": [
    "To realize this computationally, we'll implement a `Value` object as a wrapper around numbers. Numbers will be the terms in our neural network math expression. Each `Value` will know where it came from (its parents) and what operation produced it. Knowing the operation allows each value to compute its local derivative eg:\n",
    "```\n",
    "e = a + b\n",
    "```\n",
    "`e` knows it came from `a, b` through `+` so it can compute `de/da, de/db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db5cc18-3312-4e6a-86d2-ac710c753bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ab7da0-6448-4bf5-af0f-7b808a818f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's start with simple demo\n",
    "class Value:\n",
    "    def __init__(self, data, _parents=(), op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_parents)\n",
    "        self.op = op\n",
    "        self.label = label\n",
    "\n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), op='+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), op='*')\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e74ba1c-8378-4564-a6e1-2b820f5f7189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c is Value(data=6.0)\n",
      "{Value(data=4.0), Value(data=2.0)}\n"
     ]
    }
   ],
   "source": [
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "c = a + b\n",
    "\n",
    "print(f\"c is {c}\")\n",
    "print(c._prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc8bb1-2e46-426e-bc98-ff324c713903",
   "metadata": {},
   "source": [
    "Let's add the ability to **compute gradients**: \n",
    "\n",
    "A `Value` object could be created manually `a = Value(2.0)` in which case its gradient is 0 by defualt or as a result of an operation on other `Value` objects eg `c = a + b` in which case it can propagate its local gradient to its parents.\n",
    "\n",
    "Obviosuly it's a waste to compute the gradient for each `Value` so we'll implement gradient computation as a method to be called when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4318d55d-6fa7-4dc7-b14e-72d0b98ad322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _parents=(), op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_parents)\n",
    "        self.op = op\n",
    "        self.label = label\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), op='+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = 1.0 * out.grad # compute local grad = 1.0 then apply chain rule to propagate\n",
    "            other.grad = 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = other.data * out.grad\n",
    "            other.grad = self.data * out.grad\n",
    "        out._backward = _backward()\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d05a478-d6cb-4cc9-bcd6-e2c6e83a8b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=2.0), Value(data=4.0), Value(data=6.0), 0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's test\n",
    "a = Value(2.0)\n",
    "b = Value(4.0)\n",
    "c = a + b\n",
    "a, b, c, a.grad, b.grad, c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee582b23-cb3b-469e-878f-a7eacb64f799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=2.0), Value(data=4.0), Value(data=6.0), 1.0, 1.0, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad = 1 #initialize dc/dc = 1\n",
    "c._backward()\n",
    "a, b, c, a.grad, b.grad, c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5531dc8b-80b7-4624-9bbd-1a5054487e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's try another example\n",
    "a = Value(3.0, label='a')\n",
    "b = a + a   ; b.label = 'b'; b.grad = 1\n",
    "b._backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31119062-c575-466d-b6de-ae3e57f0b363",
   "metadata": {},
   "source": [
    "This is wrong. `b = a + a = 2a` so `db/da (a.grad) should = 2`\n",
    "\n",
    "We have a bug in our code. Speciafically where it says:\n",
    "```\n",
    " def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), op='+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = 1.0 * out.grad # compute local grad = 1.0 then apply chain rule to propagate\n",
    "            other.grad = 1.0 * out.grad\n",
    "```\n",
    "This will execute as follows:\n",
    "\n",
    "```\n",
    "b = a + a\n",
    "self = a\n",
    "other = a\n",
    "self.grad = 1.0 * out.grad = 1.0 * 1.0 = 1.0 -> a.grad = 1.0\n",
    "and exactly the same happens in the second step:\n",
    "other.grad = 1.0 * out.grad = 1.0 * 1.0 = 1.0 -> a.grad = 1.0\n",
    "```\n",
    "\n",
    "Whenever a node is used more than once (eg if it's involved in multiple computations), it should accumulate the gradients it receives from the various points (and not reset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12b61c7-546b-4a1a-ba8f-59e0b0cd342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the correct code:\n",
    "class Value:\n",
    "    def __init__(self, data, _parents=(), op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_parents)\n",
    "        self.op = op\n",
    "        self.label = label\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), op='+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad # now += \n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward()\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134399e3-98b5-4fc3-92bd-82c8948ac735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's retest\n",
    "a = Value(3.0, label='a')\n",
    "b = a + a   ; b.label = 'b'; b.grad = 1\n",
    "b._backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187dcdf-00aa-478b-9890-c81411531a2c",
   "metadata": {},
   "source": [
    "<details> \n",
    "\n",
    "<summary>A side not on gradient accumulation (could be skipped): </summary>\n",
    "\n",
    "\n",
    "In Multivariate Calculus, gradient accumulation refers to the process of accumulating or summing up the gradients of a multivariate function at different points. The gradient of a multivariate function is a vector that points in the direction of the steepest ascent of the function at a given point.\n",
    "\n",
    "Here's how gradient accumulation works in Multivariate Calculus:\n",
    "\n",
    "1. Consider a multivariate function f(x₁, x₂, ..., xₙ) that maps a vector of input variables (x₁, x₂, ..., xₙ) to a scalar output.\n",
    "\n",
    "2. The gradient of the function f at a point (a₁, a₂, ..., aₙ) is denoted as ∇f(a₁, a₂, ..., aₙ) and is given by:\n",
    "   ∇f(a₁, a₂, ..., aₙ) = (∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ) evaluated at (a₁, a₂, ..., aₙ)\n",
    "   \n",
    "   Where ∂f/∂xᵢ represents the partial derivative of f with respect to xᵢ.\n",
    "\n",
    "3. Gradient accumulation involves calculating the gradients of the function f at multiple points and summing them up.\n",
    "\n",
    "4. Let's say we have a set of points {(a₁, a₂, ..., aₙ), (b₁, b₂, ..., bₙ), ..., (k₁, k₂, ..., kₙ)} at which we want to accumulate the gradients.\n",
    "\n",
    "5. We calculate the gradient at each point:\n",
    "   - ∇f(a₁, a₂, ..., aₙ) = (∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ) evaluated at (a₁, a₂, ..., aₙ)\n",
    "   - ∇f(b₁, b₂, ..., bₙ) = (∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ) evaluated at (b₁, b₂, ..., bₙ)\n",
    "   - ...\n",
    "   - ∇f(k₁, k₂, ..., kₙ) = (∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ) evaluated at (k₁, k₂, ..., kₙ)\n",
    "\n",
    "6. The accumulated gradient is then obtained by summing up the gradients at each point:\n",
    "   Accumulated Gradient = ∇f(a₁, a₂, ..., aₙ) + ∇f(b₁, b₂, ..., bₙ) + ... + ∇f(k₁, k₂, ..., kₙ)\n",
    "\n",
    "The accumulated gradient represents the combined effect of the gradients at multiple points. It provides information about the overall direction and magnitude of the steepest ascent of the function considering the contributions from different points.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6caa2a-656a-40a1-a3fb-248511adc85a",
   "metadata": {},
   "source": [
    "#### Automating backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d9091-628d-45c4-af9f-e5bebba768d0",
   "metadata": {},
   "source": [
    "Let's automate this process of recursive application of the chain rule (a.k.a backpropagation a.k.a repeated calls to `._backward`\n",
    "\n",
    "To do so, we'll need to order the nodes such that we start at the last node and only call `.backward` on some node if we've computed the gradient on all its dependencies (the ones after it in the graph).\n",
    "\n",
    "\n",
    "In our simple example :\n",
    "```\n",
    "e = a * b\n",
    "d = e + c\n",
    "L = d * f\n",
    "```\n",
    "```\n",
    "a ---  \n",
    "      \\  \n",
    "       (*)-- e --(+)-- d --(*)-- L\n",
    "      /           |         |\n",
    "b ---             |         |\n",
    "                  |      f--\n",
    "              c---\n",
    "```\n",
    "A typical use case is calling `.backward()` on `L` and then the algorithm should figure out it need initialize the gradient of `L` to 1 (which represents `dL/dL`) then calls `L._backward()` then `d and f` and so on all the way to `a and b`\n",
    "\n",
    "Sorting the nodes this way (left to right) as laid out in the graph so they have the correct order can be done using Topological Sort algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbf3128e-28fd-43df-bede-73cc854d9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_sorted = []\n",
    "visited = set()\n",
    "\n",
    "def build_topo(node):\n",
    "    if node not in visited:\n",
    "        visited.add(node)\n",
    "        for parent in node._prev:\n",
    "            build_topo(parent)\n",
    "        topo_sorted.append(node) # add a node if all its parent nodes are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86b6a18d-d8f2-4c05-a7a5-6831f8107b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=1.0),\n",
       " Value(data=2.0),\n",
       " Value(data=2.0),\n",
       " Value(data=3.0),\n",
       " Value(data=5.0),\n",
       " Value(data=4.0),\n",
       " Value(data=20.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(1.0)\n",
    "b = Value(2.0)\n",
    "c = Value(3.0)\n",
    "f = Value(4.0)\n",
    "\n",
    "e = a * b\n",
    "d = e + c\n",
    "L = d * f\n",
    "\n",
    "topo_sorted = []\n",
    "visited = set()\n",
    "build_topo(L)\n",
    "topo_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae392a96-7eb2-4697-ac40-163db4b7e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add this to our code\n",
    "class Value:\n",
    "    def __init__(self, data, _parents=(), op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_parents)\n",
    "        self.op = op\n",
    "        self.label = label\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), op='+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        def build_topo(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for parent in node._prev:\n",
    "                    build_topo(parent)\n",
    "                topo.append(node)\n",
    "\n",
    "        self.grad = 1.0 #initialize last node (current)\n",
    "        build_topo(self) # build the graph topologically sorted\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ee1cbfa-5dfb-4d4d-afca-4d012aafa451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "4.0\n",
      "4.0\n",
      "4.0\n",
      "4.0\n",
      "5.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "a = Value(1.0)\n",
    "b = Value(2.0)\n",
    "c = Value(3.0)\n",
    "f = Value(4.0)\n",
    "\n",
    "e = a * b\n",
    "d = e + c\n",
    "L = d * f\n",
    "\n",
    "L.backward()\n",
    "nodes_list = [a,b,c,d,e,f,L]\n",
    "for node in nodes_list:\n",
    "    print(node.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08ba83-5aa9-4bf4-892d-93417fd88a7d",
   "metadata": {},
   "source": [
    "#### Adding more operations / functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4009a4-c7f3-4435-93d3-555111f465a6",
   "metadata": {},
   "source": [
    "Now, we can add more operations. In fact, we can add any operation / function we want as long as we know how to compute its local gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661855d-5ae1-4249-9e93-d4cdd2f12260",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary> NB: Notice what happens with `+` and `*` (<i>click to expand/collpase</i>) </summary>\n",
    "\n",
    "```\n",
    "    def __add__(self, other):\n",
    "        ...\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad \n",
    "            other.grad += 1.0 * out.grad\n",
    "        ...\n",
    "```\n",
    "`+` is just routing / distributing the gradient it receives (`out.grad`)\n",
    "\n",
    "```\n",
    "    def __mul__(self, other):\n",
    "        ...\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        ...\n",
    "```\n",
    "`*` is scaling the gradient by some factor eg `self.data` or `other.data`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fae82-a41a-4d0c-b776-41bf8628b68f",
   "metadata": {},
   "source": [
    "Other functions to implement:\n",
    "- `pow`\n",
    "- `tanh`\n",
    "- `exp`\n",
    "- ` sub` and `div`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4736040a-fc0d-49ca-a864-d60a2794bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _parents=(), op='', label=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_parents)\n",
    "        self.op = op\n",
    "        self.label = label\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data:{self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), op='+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad +=  1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), op='*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visisted = set()\n",
    "        def topo_sort(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for parent in node._prev:\n",
    "                    topo_sort(parent)\n",
    "                topo.append(node)\n",
    "\n",
    "        topo_sort(self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self, ), op=f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data** (other -1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), op='tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # subtraction is addition to negative other\n",
    "    def __neg__(self):\n",
    "        return -1 * self\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    # division is multiplication by other^-1\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    # make the operations work both ways x + y == y + x\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be2bad0d-87b3-4b22-9706-0efa81ff5bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "a : 6.0\n",
      "b : -4.0\n",
      "c : -2.0\n",
      "f : 4.0\n",
      "a: tensor(6.)\n",
      "b: tensor(-4.)\n",
      "c: tensor(-2.)\n",
      "f: tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# testing: checking against torch\n",
    "import torch\n",
    "\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a*b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L'\n",
    "L\n",
    "print(L.grad)\n",
    "L.backward()\n",
    "print(L.grad)\n",
    "nodes = [a,b,c,f]\n",
    "for node in nodes:\n",
    "    print(f\"{node.label} : {node.grad}\")\n",
    "\n",
    "\n",
    "\n",
    "at = torch.tensor(2.0, requires_grad=True) \n",
    "bt = torch.tensor(-3.0, requires_grad=True) \n",
    "ct = torch.tensor(10.0, requires_grad=True)\n",
    "et = at * bt\n",
    "dt = et + ct\n",
    "ft = torch.tensor(-2.0, requires_grad=True)\n",
    "Lt = dt * ft\n",
    "Lt.backward()\n",
    "print(\"a:\", at.grad)\n",
    "print(\"b:\", bt.grad)\n",
    "print(\"c:\", ct.grad)\n",
    "print(\"f:\", ft.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871c61a-d95a-4e1f-beaa-bfe95d0db43d",
   "metadata": {},
   "source": [
    "### Builiding a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1dd91f-7e7f-4fa3-ae4f-1fd62b4a3f17",
   "metadata": {},
   "source": [
    "The building block of NN is a Neuron. A Neuron is what it does `wx + b`.\n",
    "\n",
    "We'll model our NN on PyTorch's api:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f7e368a-086a-40d3-82c6-e5b8bae77b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2bbc41-924f-45ff-a3ce-20edcf6bafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        acts = sum([wi*xi for wi,xi in zip(self.w, x)], start=self.b)\n",
    "        out = acts.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f20d06a7-c588-4bda-a458-89a01d5e7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1., 3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28ff60fa-1e34-42e0-83e0-63503d5a0548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data:0.9999960680963254)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Neuron(3)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c65539-8469-48d6-9504-6ff8cec08a3a",
   "metadata": {},
   "source": [
    "Next we'll define a layer of neurons. A layer is a list of neurons. It has:\n",
    "- `nout`: number of neurons\n",
    "- `nin`: number of synpses / weights / inputs for each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c18c1630-f0ce-4776-8b35-2ea8319b8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs)==1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86bd9631-94eb-4fcd-baad-349f63cc8284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data:-0.9978931896651686),\n",
       " Value(data:0.9995903451151995),\n",
       " Value(data:-0.9990237619573146),\n",
       " Value(data:-0.9990817034271469),\n",
       " Value(data:-0.9996555273127059)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = Layer(3, 5)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12968dd7-13df-40ce-b567-f06a2508e71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6cefce-5cb5-4f3b-a491-5e2e3ec52598",
   "metadata": {},
   "source": [
    "Finally, a sequence of layers is MLP. It takes:\n",
    "- `nin`: num of synapses / weights / inputs for each neuron\n",
    "- `nouts`: a list of number of neurons (outputs) of each layer\n",
    "\n",
    "Notice, except for the first layer, the number of neurons in each layer `nout` will be `nin` of the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a37b3d2c-ca48-47e1-80e8-07744c7fa4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], nouts[i]) for i, _ in enumerate(nouts)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "960e8726-a782-49a6-8623-3f26e41b3a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data:0.3548070700404524)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a206d8-debc-486e-b42a-4b3fd54a2d27",
   "metadata": {},
   "source": [
    "Let's test a training and optimizing a simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "457b9de3-7554-49e8-bc68-6de0e772c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets\n",
    "\n",
    "n = MLP(3, [4, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7920bdd-214c-4285-a3e6-de17d505f5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.026851401448691387\n",
      "1: 0.025256724932848744\n",
      "2: 0.023832177002269205\n",
      "3: 0.022552429414843644\n",
      "4: 0.021396890219210117\n",
      "5: 0.020348653782859497\n",
      "6: 0.019393716910118118\n",
      "7: 0.01852038585466734\n",
      "8: 0.01771882230435686\n",
      "9: 0.016980691906418995\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum((y - yhat)**2 for y, yhat in zip(ys, ypred))\n",
    "    # loss = sum([(y - yhat)**2 for y, yhat in zip(ys, ypred)])\n",
    "    # zero gradients of previous step before backwards pass \n",
    "    for p in n.parameters():\n",
    "        p.grad = 0\n",
    "    \n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "    \n",
    "    print(f\"{i}: {loss.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e809ab-4aa4-4282-b7d6-decdf7202b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
